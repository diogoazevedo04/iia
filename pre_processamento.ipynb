{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed549ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabf8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A carregar o dataset original\n",
      "Dataset Original carregado: 515738 linhas, 17 colunas\n"
     ]
    }
   ],
   "source": [
    "print(\"A carregar o dataset original\")\n",
    "try:\n",
    "    df = pd.read_csv(\"Hotel_Reviews.csv\")\n",
    "    print(f\"Dataset Original carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERRO: O ficheiro 'Hotel_Reviews.csv' não foi encontrado. Verifica a pasta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a87f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A limpar dados ruidosos\n",
      "Limpeza de textos padrão concluída.\n"
     ]
    }
   ],
   "source": [
    "print(\"A limpar dados ruidosos...\")\n",
    "\n",
    "# O Booking.com preenche reviews vazias com \"No Negative\" ou \"No Positive\".\n",
    "# Isto confunde a IA (ela acha que a palavra \"Negative\" está no texto).\n",
    "# Substituímos por string vazia.\n",
    "df[\"Negative_Review\"] = df[\"Negative_Review\"].replace(\"No Negative\", \"\")\n",
    "df[\"Positive_Review\"] = df[\"Positive_Review\"].replace(\"No Positive\", \"\")\n",
    "\n",
    "# Substituir valores vazios reais (NaN) por string vazia\n",
    "df[\"Negative_Review\"] = df[\"Negative_Review\"].fillna(\"\")\n",
    "df[\"Positive_Review\"] = df[\"Positive_Review\"].fillna(\"\")\n",
    "\n",
    "print(\"Limpeza de textos padrão concluída.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac407e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostragem concluída!\n",
      "Número de hotéis únicos: 1460\n",
      "Total de reviews final: 46537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diogo\\AppData\\Local\\Temp\\ipykernel_18240\\387911594.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_sorted.groupby('Hotel_Name', group_keys=False).apply(sample_hotel_reviews)\n"
     ]
    }
   ],
   "source": [
    "# Conta reviews positivas e negativas por hotel\n",
    "positivas = df[df['Positive_Review'].str.strip() != ''].groupby('Hotel_Name').size()\n",
    "negativas = df[df['Negative_Review'].str.strip() != ''].groupby('Hotel_Name').size()\n",
    "\n",
    "# Seleciona hotéis com pelo menos 15 de cada\n",
    "hoteis_validos = positivas[(positivas >= 15) & (negativas.get(positivas.index, 0) >= 15)].index\n",
    "\n",
    "# Filtra o DataFrame\n",
    "df = df[df['Hotel_Name'].isin(hoteis_validos)]\n",
    "\n",
    "df = df[(df['Review_Total_Positive_Word_Counts'] > 10) & (df['Review_Total_Negative_Word_Counts'] > 10)]\n",
    "\n",
    "df['word_count'] = df['Review_Total_Positive_Word_Counts'] + df['Review_Total_Negative_Word_Counts']\n",
    "\n",
    "# Ordena por riqueza de informação para garantir que as 20 escolhidas são detalhadas\n",
    "df_sorted = df.sort_values(['Hotel_Name', 'word_count'], ascending=[True, False])\n",
    "\n",
    "def sample_hotel_reviews(group):\n",
    "    # Se o hotel tem 40 ou menos reviews, devolvemos todas\n",
    "    if len(group) <= 40:\n",
    "        return group\n",
    "    \n",
    "    # Se tem mais de 40, pegamos nas 20 com melhor nota e 20 com pior nota\n",
    "    top_20 = group.nlargest(20, 'Reviewer_Score')\n",
    "    bottom_20 = group.nsmallest(20, 'Reviewer_Score')\n",
    "    \n",
    "    # Concatenar e remover duplicados (caso o top 20 e bottom 20 se sobreponham)\n",
    "    return pd.concat([top_20, bottom_20]).drop_duplicates()\n",
    "\n",
    "# 3. Aplicar a amostragem por grupo de hotel\n",
    "df_final = df_sorted.groupby('Hotel_Name', group_keys=False).apply(sample_hotel_reviews)\n",
    "\n",
    "# 4. Baralhar os resultados finais para o modelo não ver tudo por ordem alfabética\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_final = df_final.drop(columns=['word_count'])\n",
    "\n",
    "print(f\"Amostragem concluída!\")\n",
    "print(f\"Número de hotéis únicos: {df_final['Hotel_Name'].nunique()}\")\n",
    "print(f\"Total de reviews final: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca40ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_tags(tag_str):\n",
    "    if pd.isna(tag_str) or tag_str == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    clean_str = tag_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "    \n",
    "    lista_tags = clean_str.split(\",\")\n",
    "    \n",
    "    tags_limpas = [t.strip() for t in lista_tags if t.strip()]\n",
    "    \n",
    "    # Devolver como uma string limpa separada por vírgulas ou como lista\n",
    "    return \", \".join(tags_limpas)\n",
    "\n",
    "df_final['Tags_Clean'] = df_final['Tags'].apply(limpar_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Função de processamento definida.\n"
     ]
    }
   ],
   "source": [
    "def processar_texto(row):\n",
    "    \"\"\"\n",
    "    Combina as reviews positiva e negativa e adiciona contexto (Nome do Hotel e Endereço).\n",
    "    Não removemos stopwords para manter a semântica para modelos Transformer.\n",
    "    \"\"\"\n",
    "    # Tratamento do conteúdo positivo\n",
    "    pos = str(row['Positive_Review']).strip()\n",
    "    texto_positivo = f\"O QUE OS CLIENTES ADORAM: {pos}\" if pos else \"Sem comentários positivos destacados.\"\n",
    "    \n",
    "    # Tratamento do conteúdo negativo\n",
    "    neg = str(row['Negative_Review']).strip()\n",
    "    texto_negativo = f\"PONTOS A MELHORAR: {neg}\" if neg else \"Sem queixas relevantes registadas.\"\n",
    "    \n",
    "    # ESTRATÉGIA RAG: Injetar Metadados no Texto \n",
    "    # Ajuda a encontrar \"Hotel em Londres\" porque a palavra \"London\" passa a fazer parte do texto vetorial.\n",
    "    texto_final = (\n",
    "        f\"Hotel Name: {row['Hotel_Name']}. \"\n",
    "        f\"Location: {row['Hotel_Address']}. \"\n",
    "        f\"{texto_positivo} \"\n",
    "        f\"{texto_negativo}\"\n",
    "    )\n",
    "    \n",
    "    texto_final = \" \".join(texto_final.split())\n",
    "    \n",
    "    return texto_final\n",
    "\n",
    "print(\"Função de processamento definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12903d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A criar a coluna de Texto Enriquecido\n",
      "Processamento concluído. Linhas válidas restantes: 46537\n"
     ]
    }
   ],
   "source": [
    "print(\"A criar a coluna de Texto Enriquecido...\")\n",
    "\n",
    "df_final['review'] = df_final.apply(processar_texto, axis=1)\n",
    "\n",
    "print(f\"Processamento concluído. Linhas válidas restantes: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A processar Inteligência Artificial (Keywords, NER, POIs e Cidades)...\n",
      "Concluído!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "extra_stops = {\n",
    "    'hotel', 'room', 'staff', 'stay', 'location', 'would', 'could', 'also', 'get', 'us',\n",
    "    'loved', 'liked', 'amazing', 'good', 'nice', 'excellent', 'great', 'really', 'bit',\n",
    "    'everything', 'nothing', 'breakfast', 'egg', 'eggs', 'even', 'next', 'one', 'back', \n",
    "    'front', 'desk', 'facilities', 'nearest'\n",
    "}\n",
    "ALL_STOPWORDS = stop_words_nltk.union(extra_stops)\n",
    "\n",
    "# Lista de exclusão definitiva para o NER (Palavras que NUNCA devem ser entidades)\n",
    "STOP_NER = {\n",
    "    'breakfast', 'room', 'rooms', 'bed', 'beds', 'shower', 'bathroom', \n",
    "    'staff', 'hotel', 'stay', 'everything', 'nothing', 'thing', 'bit',\n",
    "    'nice', 'great', 'small', 'tiny', 'excellent', 'door', 'window',\n",
    "    'front', 'desk','facilities', 'nearest', 'egg', 'eggs', 'loved', \n",
    "    'really', 'helpful', 'liked'\n",
    "}\n",
    "\n",
    "def extrair_conhecimento_hibrido(row):\n",
    "    texto_pos = str(row['Positive_Review'])\n",
    "    texto_neg = str(row['Negative_Review'])\n",
    "    texto_total = (texto_pos + \" \" + texto_neg).strip()\n",
    "    \n",
    "    if len(texto_total) < 10:\n",
    "        return \"\", \"\", \"\"\n",
    "\n",
    "    # Keywords\n",
    "    # Extraímos as palavras mais frequentes que não são stopwords\n",
    "    palavras = re.findall(r'\\b[a-zA-Z]{3,}\\b', texto_total.lower())\n",
    "    palavras_uteis = [p for p in palavras if p not in ALL_STOPWORDS]\n",
    "    keywords = \", \".join([w for w, f in Counter(palavras_uteis).most_common(8)])\n",
    "\n",
    "    # NER & POIs\n",
    "    doc = nlp(texto_total[:1500])\n",
    "\n",
    "    entidades_geral = []\n",
    "    pois = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        ent_text = ent.text.strip()\n",
    "        ent_lower = ent_text.lower()\n",
    "        \n",
    "        # Ignora se a base for Adjetivo/Verbo ou se estiver na lista STOP_NER\n",
    "        if ent.root.pos_ in ['ADJ', 'VERB'] or ent_lower in STOP_NER:\n",
    "            continue\n",
    "        \n",
    "        # Ignora se for o próprio nome do hotel (evita auto-referência)\n",
    "        if ent_lower in str(row['Hotel_Name']).lower():\n",
    "            continue\n",
    "\n",
    "        # Coluna POI: Apenas locais físicos e natureza (FAC e LOC)\n",
    "        if ent.label_ in ['FAC', 'LOC']:\n",
    "            pois.append(ent_text)\n",
    "            # Se é um POI, tem de estar obrigatoriamente nas entidades_ner\n",
    "            entidades_geral.append(f\"{ent_text} ({ent.label_})\")\n",
    "\n",
    "        # Coluna Entidades: Adiciona também as Organizações e Cidades\n",
    "        elif ent.label_ in ['ORG', 'GPE']:\n",
    "            entidades_geral.append(f\"{ent_text} ({ent.label_})\")\n",
    "\n",
    "    return (\n",
    "        keywords,\n",
    "        \", \".join(list(dict.fromkeys(entidades_geral))[:5]),\n",
    "        \", \".join(list(dict.fromkeys(pois))[:4])\n",
    "    )\n",
    "\n",
    "# Função para verificar a cidade no endereço\n",
    "def identificar_cidade(address):\n",
    "    for city in [\"London\", \"Paris\", \"Amsterdam\", \"Barcelona\", \"Milan\", \"Vienna\"]:\n",
    "        if city in address or (city == \"London\" and \"United Kingdom\" in address):\n",
    "            return city\n",
    "    return \"Other\"\n",
    "\n",
    "print(\"A processar Keywords, NER, POIs e Cidades...\")\n",
    "\n",
    "df_final[['keywords', 'entidades_ner', 'POI']] = df_final.apply(\n",
    "    lambda row: pd.Series(extrair_conhecimento_hibrido(row)), axis=1\n",
    ")\n",
    "\n",
    "df_final['City'] = df_final['Hotel_Address'].apply(identificar_cidade)\n",
    "\n",
    "print(\"Concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f2ccbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_uteis = [\n",
    "    'Hotel_Name', \n",
    "    'Hotel_Address', \n",
    "    'City',\n",
    "    'Average_Score',\n",
    "    'Total_Number_of_Reviews',\n",
    "    'Reviewer_Nationality',\n",
    "    'review',\n",
    "    'Review_Date',\n",
    "    'Positive_Review', \n",
    "    'Negative_Review',\n",
    "    'Reviewer_Score',\n",
    "    'Tags_Clean',\n",
    "    'keywords',\n",
    "    'entidades_ner',\n",
    "    'POI',\n",
    "    'lat', \n",
    "    'lng'\n",
    "]\n",
    "\n",
    "df_final = df_final[colunas_uteis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466019f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gerar embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871762bba33e42bdb0b9e21fda9ad9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diogo\\AppData\\Local\\Temp\\ipykernel_18240\\3204215337.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['embeddings'] = list(embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucesso! Agora usa o ficheiro .pkl na tua app.py\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "print(\"A gerar embeddings...\")\n",
    "embeddings = model.encode(df_final['review'].tolist(), show_progress_bar=True)\n",
    "\n",
    "df_final['embeddings'] = list(embeddings)\n",
    "\n",
    "df_final.to_pickle(\"Hotel_Reviews_processed.pkl\")\n",
    "print(\"Sucesso! O ficheiro .pkl está pronto para ser usado na app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff9176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A guardar ficheiro final...\n",
      "Csv criado\n"
     ]
    }
   ],
   "source": [
    "print(f\"A guardar ficheiro final...\")\n",
    "df_final.to_csv(\"Hotel_Reviews_processed.csv\", index=False)\n",
    "print(\"Csv criado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
