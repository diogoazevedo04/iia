{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "     ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 3.4/13.0 MB 21.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/13.0 MB 21.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.6/13.0 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 13.0/13.0 MB 20.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip -q install spacy ftfy unidecode wordcloud matplotlib pandas scikit-learn lxml\n",
    "!python -m spacy download en_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpeza de texto:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, html, json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#POR AQUILO QUE VI, NAO EXISTEM NENHUM DOMAIN TRASH A REMOVER, nao sei se entao metemos ou nao \n",
    "\n",
    "\n",
    "def basic_clean(text: str,\n",
    "                lower=True,\n",
    "                strip_accents=True,   \n",
    "                keep_hyphen=True) -> str:  #preserva os hifens\n",
    "    \"\"\"Basic text cleaning for English hotel reviews.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    t = text\n",
    "\n",
    "    # corrigir problemas de encoding\n",
    "    t = fix_text(t)\n",
    "\n",
    "    # remove tags HTML(apesar de nao ter encontrado nenhuma)\n",
    "    t = re.sub(r\"<[^>]+>\", \" \", t)\n",
    "\n",
    "    # remove URLs(apesar de nao ter encontrado nenhuma )\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", t)\n",
    "\n",
    "    #remove nao caracteres do dataset\n",
    "    if keep_hyphen:\n",
    "        t = re.sub(r\"[^0-9A-Za-z\\- ]\", \" \", t)\n",
    "    else:\n",
    "        t = re.sub(r\"[^0-9A-Za-z ]\", \" \", t)\n",
    "\n",
    "    # normalizar hífens e espaços\n",
    "    t = re.sub(r\"-{2,}\", \"-\", t)\n",
    "    t = re.sub(r\"\\s*-\\s*\", \"-\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # lowercase\n",
    "    if lower:\n",
    "        t = t.lower()\n",
    "\n",
    "    # remove acentos \n",
    "    if strip_accents:\n",
    "        t = unidecode(t)\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def pre_clean(s: str) -> str:\n",
    "    \"\"\"Corrigir unicode + limpar.\"\"\" # nao meti aqui para decodificar entidades HTML, uma vez que nao temos \n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if s is None else str(s)\n",
    "    t = fix_text(s)\n",
    "    t = basic_clean(t, lower=True, strip_accents=False, keep_hyphen=True)\n",
    "    return t\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leitura do csv:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hotel_Reviews.csv\")\n",
    "\n",
    "# Negative reviews\n",
    "df[\"clean_negative\"] = df[\"Negative_Review\"].apply(pre_clean)\n",
    "\n",
    "# Positive reviews\n",
    "df[\"clean_positive\"] = df[\"Positive_Review\"].apply(pre_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparação de colunas alteradas com as originais:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Negative_Review  \\\n",
      "0   I am so angry that i made this post available...   \n",
      "1                                        No Negative   \n",
      "2   Rooms are nice but for elderly a bit difficul...   \n",
      "3   My room was dirty and I was afraid to walk ba...   \n",
      "4   You When I booked with your company on line y...   \n",
      "5   Backyard of the hotel is total mess shouldn t...   \n",
      "6   Cleaner did not change our sheet and duvet ev...   \n",
      "7   Apart from the price for the brekfast Everyth...   \n",
      "8   Even though the pictures show very clean room...   \n",
      "9   The aircondition makes so much noise and its ...   \n",
      "\n",
      "                                      clean_negative  \n",
      "0  i am so angry that i made this post available ...  \n",
      "1                                        no negative  \n",
      "2  rooms are nice but for elderly a bit difficult...  \n",
      "3  my room was dirty and i was afraid to walk bar...  \n",
      "4  you when i booked with your company on line yo...  \n",
      "5  backyard of the hotel is total mess shouldn t ...  \n",
      "6  cleaner did not change our sheet and duvet eve...  \n",
      "7  apart from the price for the brekfast everythi...  \n",
      "8  even though the pictures show very clean rooms...  \n",
      "9  the aircondition makes so much noise and its h...  \n",
      "                                     Positive_Review  \\\n",
      "0   Only the park outside of the hotel was beauti...   \n",
      "1   No real complaints the hotel was great great ...   \n",
      "2   Location was good and staff were ok It is cut...   \n",
      "3   Great location in nice surroundings the bar a...   \n",
      "4    Amazing location and building Romantic setting    \n",
      "5   Good restaurant with modern design great chil...   \n",
      "6   The room is spacious and bright The hotel is ...   \n",
      "7   Good location Set in a lovely park friendly s...   \n",
      "8                                        No Positive   \n",
      "9   The room was big enough and the bed is good T...   \n",
      "\n",
      "                                      clean_positive  \n",
      "0   only the park outside of the hotel was beautiful  \n",
      "1  no real complaints the hotel was great great l...  \n",
      "2  location was good and staff were ok it is cute...  \n",
      "3  great location in nice surroundings the bar an...  \n",
      "4     amazing location and building romantic setting  \n",
      "5  good restaurant with modern design great chill...  \n",
      "6  the room is spacious and bright the hotel is l...  \n",
      "7  good location set in a lovely park friendly st...  \n",
      "8                                        no positive  \n",
      "9  the room was big enough and the bed is good th...  \n"
     ]
    }
   ],
   "source": [
    "# Ver só as linhas onde a review negativa mudou após limpeza\n",
    "diff_neg = df[df[\"Negative_Review\"] != df[\"clean_negative\"]]\n",
    "print(diff_neg[[\"Negative_Review\", \"clean_negative\"]].head(10))  # mostra as primeiras 10 diferenças\n",
    "\n",
    "# Para reviews positivas\n",
    "diff_pos = df[df[\"Positive_Review\"] != df[\"clean_positive\"]]\n",
    "print(diff_pos[[\"Positive_Review\", \"clean_positive\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "EN_STOP = nlp.Defaults.stop_words #default stop words em ingles\n",
    "\n",
    "def spacy_tokenize_lemmatize(doc_text: str,\n",
    "                             remove_stop=True,\n",
    "                             only_alpha=True,\n",
    "                             min_len=3) -> list[str]:\n",
    "    \"\"\"Tokeniza com spaCy e devolve lemas filtrados.\"\"\"\n",
    "    if not isinstance(doc_text, str) or not doc_text.strip():\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(doc_text)\n",
    "    out = []\n",
    "    for tok in doc:\n",
    "        lemma = tok.lemma_ if tok.lemma_ != \"\" else tok.text\n",
    "        if remove_stop and lemma.lower() in EN_STOP:\n",
    "            continue\n",
    "        if only_alpha and not lemma.isalpha():\n",
    "            continue\n",
    "        if len(lemma) < min_len:\n",
    "            continue\n",
    "        out.append(lemma.lower())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lê o ficheiro\n",
    "df = pd.read_csv(\"Hotel_Reviews.csv\")\n",
    "\n",
    "# Combinar as colunas num único texto\n",
    "df[\"review\"] = df[\"Positive_Review\"].fillna(\"\") + \" \" + df[\"Negative_Review\"].fillna(\"\")\n",
    "\n",
    "# (Opcional) remover espaços duplos\n",
    "df[\"review\"] = df[\"review\"].str.strip()\n",
    "\n",
    "# df = df.drop(columns=[\"positive_review\", \"negative_review\"]) -> ver se fazemos drop ou nao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo tokens do documento 0: ['park', 'outside', 'hotel', 'beautiful', 'angry', 'post', 'available', 'possible', 'site', 'use', 'plane', 'trip', 'mistake', 'book', 'place', 'booking', 'book', 'com', 'stay', 'night']\n"
     ]
    }
   ],
   "source": [
    "# Limpeza\n",
    "df[\"clean_text\"] = df[\"review\"].apply(pre_clean)\n",
    "\n",
    "# Tokenização + lematização\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(lambda t: spacy_tokenize_lemmatize(\n",
    "    t, remove_stop=True, only_alpha=True, min_len=3\n",
    "))\n",
    "\n",
    "print(\"Exemplo tokens do documento 0:\", df[\"tokens\"].iloc[0][:20])\n",
    "\n",
    "# Guardar o resultado\n",
    "df.to_csv(\"Hotel_Reviews_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # Para veres a barra de progresso\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"A carregar dados...\")\n",
    "df = pd.read_csv(\"Hotel_Reviews_processed.csv\")\n",
    "\n",
    "# Se quiseres testar rápido, descomenta a linha abaixo para fazer só 100 linhas\n",
    "df = df.head(100)\n",
    "\n",
    "print(\"A gerar embeddings com Ollama (isto vai demorar)...\")\n",
    "# Função para aplicar a cada linha\n",
    "def get_emb(text):\n",
    "    try:\n",
    "        # Certifica-te que tens o 'ollama serve' ligado no terminal!\n",
    "        return ollama.embeddings(model='nomic-embed-text', prompt=text)['embedding']\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Cria a coluna nova com os números\n",
    "df['embeddings'] = df['review'].progress_apply(get_emb)\n",
    "\n",
    "# Guarda num ficheiro novo (PICKLE é melhor que CSV para guardar vetores/listas)\n",
    "print(\"A guardar ficheiro pronto...\")\n",
    "df.to_pickle(\"dados_com_embeddings.pkl\")\n",
    "print(\"Concluído!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
